{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import groupby\n",
    "import math\n",
    "import pickle\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 372 ms, sys: 35.8 ms, total: 408 ms\n",
      "Wall time: 609 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# df = pd.read_excel(\"IR00_dataset_ph3/IR00_3_11k News.xlsx\")\n",
    "df = pd.read_csv(\"IR00_dataset_ph3/IR00_3_11k News.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(token):\n",
    "    token = token.strip(\".ء \\u200b\\u200c\\u200d\")\n",
    "    token = re.sub(\"[ـ،؛,–—!٪؟+:_٫/.]\", \" \", token)\n",
    "    token = re.sub(\"[*%&…#=\\\\\\•;!|\\-?]\", \" \", token)\n",
    "    token = re.sub(\"[﴾﴿«»()<>\\[\\]“”'\\\"]\", \" \", token)\n",
    "    token = re.sub(\"[\\u200f\\u200e\\ufeff\\u2067\\u202a\\u202b\\u202c\\u2069\\xad]\", \" \", token)\n",
    "    token = re.sub(\"\\s+\", \" \", token)\n",
    "    \n",
    "    plurals = {\n",
    "        \"آداب\": \"ادب\", \"اطراف\": \"طرف\", \"حقایق\": \"حقیقت\", \"امواج\": \"موج\",\n",
    "        \"مراکز\": \"مرکز\", \"اعماق\": \"عمق\", \"مواقع\": \"موقع\", \"اخبار\": \"خبر\",\n",
    "        \"علما\": \"عالم\", \"آثار\": \"اثر\", \"مصارف\": \"مصرف\", \"علوم\": \"علم\",\n",
    "        \"ادیان\": \"دین\", \"علائم\": \"علامت\", \"اسامی\": \"اسم\", \"مباحث\": \"مبحث\",\n",
    "        \"دفاتر\": \"دفتر\", \"علل\": \"علت\", \"مذاهب\": \"مذهب\", \"عناصر\": \"عنصر\",\n",
    "        \"مساجد\": \"مسجد\", \"روابط\": \"رابطه\", \"اعضا\": \"عضو\", \"عبارات\": \"عبارت\",\n",
    "        \"موارد\": \"مورد\", \"مفاهیم\": \"مفهوم\", \"اشعار\": \"اشعار\", \"منابع\": \"منبع\",\n",
    "        \"منبع\": \"قاعده\", \"فقها\": \"فقیه\", \"عجایب\": \"عجیب\", \"تصاویر\": \"تصویر\"\n",
    "    }\n",
    "    \n",
    "    for key, value in plurals.items():\n",
    "        if key in token:\n",
    "            token = token.replace(key, value)\n",
    "            \n",
    "    arabic_chars = ['\\u064B', '\\u064C', '\\u064D', '\\u064E', '\\u064F', '\\u0650', '\\u0651', '\\u0652']\n",
    "    token = re.sub(\"|\".join(arabic_chars), \"\", token)\n",
    "    \n",
    "    postfixes_a = [\"ها\", \"های\", \"هایی\", \"ی\", \"ان\",\n",
    "                   \"تر\", \"ترین\",\n",
    "                   \"گر\"]\n",
    "    postfixes_b = [\"\\u200bها\", \"\\u200cها\", \"\\u200bهای\", \"\\u200cهای\", \n",
    "                   \"\\u200bهایی\", \"\\u200cهایی\",\n",
    "                   \"\\u200bشده\", \"\\u200cشده\",\n",
    "                   \"\\u200bساز\", \"\\u200cساز\",\n",
    "                   \"\\u200bکنندگان\", \"\\u200cکنندگان\",\n",
    "                   \"\\u200bتر\", \"\\u200cتر\", \"\\u200bترین\", \"\\u200cترین\"]\n",
    "    \n",
    "    stemming_b = [\"می\\u200b\", \"می\\u200c\", \"نمی\\u200b\", \"نمی\\u200c\"]\n",
    "    stemming_e = [\"ات\", \"ام\", \"اش\", \n",
    "                  \"یم\", \"ید\", \"ند\", \n",
    "                  \"مان\", \"تان\", \"شان\"]\n",
    "    \n",
    "    prefixes = [\"با\", \"بی\", \"نا\"]\n",
    "    \n",
    "    # idea: avoid wrong cut with half space\n",
    "    token = re.sub(\"|\".join([f\"^{st}\" for st in stemming_b]), \" \", token)\n",
    "    token = re.sub(\"|\".join([f\"{pf}$\" for pf in postfixes_b]), \" \", token)\n",
    "    \n",
    "    if len(token) > 5: # idea: avoid wrong cut with char limit\n",
    "        token = re.sub(\"|\".join([f\"{pf}$\" for pf in postfixes_a]), \" \", token)\n",
    "        token = re.sub(\"|\".join([f\"{st}$\" for st in stemming_e]), \" \", token)\n",
    "        token = re.sub(\"|\".join([f\"^{pf}\" for pf in prefixes]), \" \", token)\n",
    "    \n",
    "    fa_digits = \"۱۲۳۴۵۶۷۸۹۰١٢٣٤٥٦٧٨٩٠\"\n",
    "    en_digits = \"12345678901234567890\"\n",
    "    token = token.translate(str.maketrans(fa_digits, en_digits))\n",
    "\n",
    "    token = re.sub(\"[ئي]\", \"ی\", token)\n",
    "    token = token.strip(\".ء \\u200b\\u200c\\u200d\")\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row):\n",
    "    tokens = np.array(row.content.split())\n",
    "    doc_id = np.full((len(tokens)), row.id)\n",
    "    return np.column_stack((tokens, doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.59 s, sys: 432 ms, total: 5.02 s\n",
      "Wall time: 5.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# saved\n",
    "tokenized_docs = []\n",
    "for tokens_doc in df.apply(tokenize, axis=1):\n",
    "    tokenized_docs.extend(tokens_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 10s, sys: 632 ms, total: 2min 10s\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# saved\n",
    "normalized_docs = []\n",
    "for [token, doc] in tokenized_docs:\n",
    "    token = normalize(normalize(token))\n",
    "    normalized_docs.append([token, doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.06 s, sys: 56 ms, total: 2.12 s\n",
      "Wall time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# saved\n",
    "tokens2docs = sorted(normalized_docs, key=lambda token_doc: token_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from 66985 items...\n",
      "CPU times: user 9.61 s, sys: 1.26 s, total: 10.9 s\n",
      "Wall time: 9.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# saved\n",
    "inverted_indexes = {}\n",
    "docs2tokens = {}\n",
    "print(f\"from {len(list(groupby(tokens2docs, key=lambda tokens2doc: tokens2doc[0])))} items...\")\n",
    "item = 0\n",
    "for [token, t_docs] in groupby(tokens2docs, key=lambda tokens2doc: tokens2doc[0]):\n",
    "    print(item, end=\"\\r\")\n",
    "    item += 1\n",
    "    docs = sorted(map(lambda item: int(item[1]), t_docs))\n",
    "    \n",
    "    tf = {}\n",
    "    for doc in docs:\n",
    "        if doc in tf:\n",
    "            tf[doc] += 1\n",
    "        else:\n",
    "            tf[doc] = 1\n",
    "    \n",
    "    if len(tf) > 1300:\n",
    "        continue\n",
    "    elif re.sub(\"\\s+\", \"\", token).isdigit() and len(re.sub(\"\\s+\", \"\", token)) < 4:\n",
    "        continue\n",
    "    elif re.match(\"^http|^https|^video\", token) and len(token) > 15:\n",
    "        continue\n",
    "    elif len(token) < 2:\n",
    "        continue\n",
    "    else:\n",
    "        for doc in tf.keys():\n",
    "            if doc in docs2tokens:\n",
    "                docs2tokens[doc] += [token]\n",
    "            else:\n",
    "                docs2tokens[doc] = [token]\n",
    "        inverted_indexes.update({ token: tf })\n",
    "\n",
    "inverted_indexes_file = open(\"inverted_indexes\", \"ab\")\n",
    "pickle.dump(inverted_indexes, inverted_indexes_file)\n",
    "inverted_indexes_file.close()\n",
    "\n",
    "docs2tokens_file = open(\"docs2tokens\", \"ab\")\n",
    "pickle.dump(docs2tokens, docs2tokens_file)\n",
    "docs2tokens_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_indexes_file = open(\"inverted_indexes\", \"rb\")\n",
    "inverted_indexes = pickle.load(inverted_indexes_file)\n",
    "\n",
    "docs2tokens_file = open(\"docs2tokens\", \"rb\")\n",
    "docs2tokens = pickle.load(docs2tokens_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    if row.id not in docs2tokens:\n",
    "        docs2tokens[row.id] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_dict = {token: math.log(len(df) / len(docs)) for token, docs in inverted_indexes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26 ms, sys: 110 µs, total: 26.1 ms\n",
      "Wall time: 26.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "token2index = {}\n",
    "index2token = [0 for i in range(len(inverted_indexes))]\n",
    "for idx, token in enumerate(inverted_indexes.keys()):\n",
    "    token2index[token] = idx\n",
    "    index2token[idx] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 453 ms, sys: 19.7 ms, total: 473 ms\n",
      "Wall time: 473 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inverted_indexes_tf_idf = {}\n",
    "for [token, docs_dict] in inverted_indexes.items():\n",
    "    tf = {}\n",
    "    for [doc, freq] in docs_dict.items():\n",
    "        tf[doc] = (1 + math.log(freq)) * idf_dict[token]\n",
    "    inverted_indexes_tf_idf[token] = tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 554 ms, sys: 0 ns, total: 554 ms\n",
      "Wall time: 554 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# saved\n",
    "def compute_length(doc_id):\n",
    "    length = 0\n",
    "    tokens = docs2tokens[doc_id]\n",
    "    for token in tokens:\n",
    "        length += inverted_indexes_tf_idf[token][doc_id]\n",
    "    return length\n",
    "df[\"length\"] = df[\"id\"].apply(compute_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved\n",
    "df[\"cluster\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved\n",
    "df.to_csv(\"IR00_dataset_ph3/IR00_3_11k News.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved\n",
    "centroids_id = df.sample(10)[\"id\"].values\n",
    "centroids = []\n",
    "for centroid_id in centroids_id:\n",
    "    tokens = docs2tokens[centroid_id]\n",
    "    doc_tf_idf = np.zeros(len(inverted_indexes))\n",
    "    for token in tokens:\n",
    "        doc_tf_idf[token2index[token]] = inverted_indexes_tf_idf[token][centroid_id]\n",
    "    centroids.append(doc_tf_idf)\n",
    "centroids = np.array(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_cluster(row):\n",
    "    global item\n",
    "    print(f\"row {item + 1}\", end=\"\\r\")\n",
    "    item += 1\n",
    "    similarities = np.zeros(len(centroids))\n",
    "        \n",
    "    tokens = docs2tokens[row.id]\n",
    "    for index in range(len(centroids[0])):\n",
    "        token = index2token[index]\n",
    "        if token in tokens:\n",
    "            token_tf_idf = inverted_indexes_tf_idf[token][row.id]\n",
    "            similarities += centroids[:, index] * token_tf_idf\n",
    "    \n",
    "    similarities /= np.linalg.norm(centroids)\n",
    "    return np.argmax(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 6min 59s, sys: 52.3 s, total: 1h 7min 52s\n",
      "Wall time: 19min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# saved\n",
    "item = 0\n",
    "df[\"cluster\"] = df.apply(find_cluster, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved\n",
    "df.to_csv(\"IR00_dataset_ph3/IR00_3_11k News.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved\n",
    "def update_centroids(centroids):\n",
    "    item = 0\n",
    "    print(f\"embedding size: {len(inverted_indexes_tf_idf)}\")\n",
    "    new_centroids = np.zeros(centroids.shape)\n",
    "    for token, docs_tf_idf in inverted_indexes_tf_idf.items():\n",
    "        print(item, end=\"\\r\")\n",
    "        item += 1\n",
    "        index = token2index[token]\n",
    "        for doc, tf_idf in docs_tf_idf.items():\n",
    "            new_centroids[int(df[df[\"id\"] == doc][\"cluster\"]), index] += tf_idf\n",
    "    new_centroids /= np.array(list(df.groupby(\"cluster\")[\"cluster\"].value_counts()))[:, None]\n",
    "    print(\"update centroids finished\")\n",
    "    return new_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "embedding size: 62582\n",
      "update centroids finished\n",
      "doc size: 11437\n",
      "find cluster finished\n",
      "iteration 2\n",
      "embedding size: 62582\n",
      "update centroids finished\n",
      "doc size: 11437\n",
      "find cluster finished\n",
      "iteration 3\n",
      "embedding size: 62582\n",
      "update centroids finished\n",
      "doc size: 11437\n",
      "find cluster finished\n",
      "iteration 4\n",
      "embedding size: 62582\n",
      "update centroids finished\n",
      "doc size: 11437\n",
      "find cluster finished\n"
     ]
    }
   ],
   "source": [
    "# saved\n",
    "for i in range(4):\n",
    "    print(f\"iteration {i + 1}\")\n",
    "    centroids = update_centroids(centroids)\n",
    "    item = 0\n",
    "    print(f\"doc size: {len(df)}\")\n",
    "    df[\"cluster\"] = df.apply(find_cluster, axis=1)\n",
    "    print(\"find cluster finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved\n",
    "df.to_csv(\"IR00_dataset_ph3/IR00_3_11k News.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved\n",
    "centroids_file = open(\"centroids\", \"ab\")\n",
    "pickle.dump(centroids, centroids_file)\n",
    "centroids_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_file = open(\"centroids\", \"rb\")\n",
    "centroids = pickle.load(centroids_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster  cluster\n",
       "0        0          1422\n",
       "1        1          1921\n",
       "2        2          1594\n",
       "3        3           947\n",
       "4        4           511\n",
       "5        5           877\n",
       "6        6           467\n",
       "7        7           401\n",
       "8        8           274\n",
       "9        9          3023\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"cluster\")[\"cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    similarity = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    if np.isnan(similarity):\n",
    "        return 0\n",
    "    else:\n",
    "        return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    k = 10\n",
    "    tokens = []\n",
    "    for token in query.split():\n",
    "        tokens.append(normalize(normalize(token)))\n",
    "    \n",
    "    q_tf = {}\n",
    "    for token in tokens:\n",
    "        if token in q_tf:\n",
    "            q_tf[token] += 1\n",
    "        else:\n",
    "            q_tf[token] = 1\n",
    "   \n",
    "    q_tf_idf = np.zeros(len(inverted_indexes))\n",
    "    for token in tokens:\n",
    "        if token in token2index:\n",
    "            q_tf_idf[token2index[token]] = (1 + math.log(q_tf[token])) * idf_dict[token]\n",
    "        \n",
    "    similarities = []\n",
    "    for centroid in centroids:\n",
    "        similarities.append(cosine_similarity(centroid, q_tf_idf))\n",
    "        \n",
    "    docs = df[df[\"cluster\"] == np.argmax(similarities)]\n",
    "\n",
    "    result = {}\n",
    "    found_token = []\n",
    "    for token in tokens:\n",
    "        if token in inverted_indexes:\n",
    "            found_token.append(token)\n",
    "            docs = inverted_indexes[token].keys()\n",
    "            doc_tf_idf = inverted_indexes_tf_idf[token]\n",
    "            qt_tf_idf = q_tf_idf[token2index[token]]\n",
    "            \n",
    "            for doc in docs:              \n",
    "                if doc in result:\n",
    "                    result[doc] += doc_tf_idf[doc] * qt_tf_idf\n",
    "                else:\n",
    "                    result.update({ doc: doc_tf_idf[doc] * qt_tf_idf })\n",
    "              \n",
    "    for doc in result:\n",
    "        result[doc] /= df.loc[df[\"id\"] == doc, \"length\"].values[0]\n",
    "    \n",
    "    res_df = df.loc[df[\"id\"].isin(list(result.keys())), [\"id\", \"url\"]].copy()\n",
    "    res_df[\"rank\"] = res_df[\"id\"].apply(lambda id: result[id])\n",
    "\n",
    "    final_df = res_df.loc[res_df[\"rank\"].isin(heapq.nlargest(k, list(res_df[\"rank\"])))].sort_values(by=[\"rank\"], ascending=False)\n",
    "    \n",
    "    print(found_token)\n",
    "    for index, row in final_df.iterrows():\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"#Doc {row.id}\")\n",
    "            print(row.url)\n",
    "            print(row[\"rank\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['اعزام', 'کارو', 'اردوی', 'راهی', 'نور']\n",
      "================================================================================\n",
      "#Doc 9702\n",
      "https://www.isna.ir/news/98021106170/حضور-لرستانی-ها-در-آیین-افتتاحیه-اردوهای-راهیان-نور-غرب-کشور\n",
      "0.5887203775719336\n",
      "================================================================================\n",
      "#Doc 10656\n",
      "https://www.isna.ir/news/98080904715/مراسم-افتتاحیه-راهیان-نور-دانش-آموزی-برگزار-می-شود\n",
      "0.5392010695960446\n",
      "================================================================================\n",
      "#Doc 9258\n",
      "https://www.isna.ir/news/99111410631/اهدای-نشان-ملی-خادمی-شهدا-به-سردار-مطهری\n",
      "0.509420560187563\n",
      "================================================================================\n",
      "#Doc 10031\n",
      "https://www.isna.ir/news/98041910370/خوزستانی-ها-به-مناطق-عملیاتی-غرب-کشور-می-روند\n",
      "0.49476017278910245\n",
      "================================================================================\n",
      "#Doc 10043\n",
      "https://www.isna.ir/news/98041910370/خوزستانی-ها-به-مناطق-عملیاتی-غرب-کشور-می-روند\n",
      "0.49476017278910245\n",
      "================================================================================\n",
      "#Doc 9934\n",
      "https://www.isna.ir/news/98032712899/حضور-دانشجویان-چهارمحال-و-بختیاری-در-یادمان-های-دفاع-مقدس\n",
      "0.4833069266594536\n",
      "================================================================================\n",
      "#Doc 10318\n",
      "https://www.isna.ir/news/98061005201/کارگاه-توانمندی-سازی-مدیران-کاروان-های-راهیان-نور-برگزار-شد\n",
      "0.4781446686771322\n",
      "================================================================================\n",
      "#Doc 10025\n",
      "https://www.isna.ir/news/98041708948/۲۵-هزار-دانش-آموز-کردستانی-به-راهیان-نور-می-روند\n",
      "0.462709744090279\n",
      "================================================================================\n",
      "#Doc 8518\n",
      "https://www.isna.ir/news/99070806230/۱۵-یادمان-دفاع-مقدس-به-عنوان-آثار-ملی-دفاع-مقدس-ثبت-شده-است\n",
      "0.4264667590698517\n",
      "================================================================================\n",
      "#Doc 9588\n",
      "https://www.isna.ir/news/98012208823/رشد-۳۰-درصدی-اعزام-زائران-راهیان-نور-در-کرمان\n",
      "0.3997934168431321\n",
      "CPU times: user 343 ms, sys: 0 ns, total: 343 ms\n",
      "Wall time: 314 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# search_phase2(\"تمرینات تیم تکواندو\")\n",
    "# search(\"تیم ملی تکواندو\")\n",
    "# search(\"قهرمانی پرسپولیس\")\n",
    "# search(\"سرمایه گذاری در بازار بورس\")\n",
    "search(\"اعزام کاروان های اردوی راهیان نور\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
