{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import groupby\n",
    "import math\n",
    "import pickle\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.53 s, sys: 45.2 ms, total: 1.58 s\n",
      "Wall time: 2.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_excel(\"IR00_dataset_ph3/IR00_3_11k News.xlsx\")\n",
    "# df_un_spvs = pd.read_excel(\"../IR_Spring2021_ph12_7k.xlsx\")\n",
    "df_un_spvs = pd.read_csv(\"IR_Spring2021_ph12_7k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(token):\n",
    "    token = token.strip(\".ء \\u200b\\u200c\\u200d\")\n",
    "    token = re.sub(\"[ـ،؛,–—!٪؟+:_٫/.]\", \" \", token)\n",
    "    token = re.sub(\"[*%&…#=\\\\\\•;!|\\-?]\", \" \", token)\n",
    "    token = re.sub(\"[﴾﴿«»()<>\\[\\]“”'\\\"]\", \" \", token)\n",
    "    token = re.sub(\"[\\u200f\\u200e\\ufeff\\u2067\\u202a\\u202b\\u202c\\u2069\\xad]\", \" \", token)\n",
    "    token = re.sub(\"\\s+\", \" \", token)\n",
    "    \n",
    "    plurals = {\n",
    "        \"آداب\": \"ادب\", \"اطراف\": \"طرف\", \"حقایق\": \"حقیقت\", \"امواج\": \"موج\",\n",
    "        \"مراکز\": \"مرکز\", \"اعماق\": \"عمق\", \"مواقع\": \"موقع\", \"اخبار\": \"خبر\",\n",
    "        \"علما\": \"عالم\", \"آثار\": \"اثر\", \"مصارف\": \"مصرف\", \"علوم\": \"علم\",\n",
    "        \"ادیان\": \"دین\", \"علائم\": \"علامت\", \"اسامی\": \"اسم\", \"مباحث\": \"مبحث\",\n",
    "        \"دفاتر\": \"دفتر\", \"علل\": \"علت\", \"مذاهب\": \"مذهب\", \"عناصر\": \"عنصر\",\n",
    "        \"مساجد\": \"مسجد\", \"روابط\": \"رابطه\", \"اعضا\": \"عضو\", \"عبارات\": \"عبارت\",\n",
    "        \"موارد\": \"مورد\", \"مفاهیم\": \"مفهوم\", \"اشعار\": \"اشعار\", \"منابع\": \"منبع\",\n",
    "        \"منبع\": \"قاعده\", \"فقها\": \"فقیه\", \"عجایب\": \"عجیب\", \"تصاویر\": \"تصویر\"\n",
    "    }\n",
    "    \n",
    "    for key, value in plurals.items():\n",
    "        if key in token:\n",
    "            token = token.replace(key, value)\n",
    "            \n",
    "    arabic_chars = ['\\u064B', '\\u064C', '\\u064D', '\\u064E', '\\u064F', '\\u0650', '\\u0651', '\\u0652']\n",
    "    token = re.sub(\"|\".join(arabic_chars), \"\", token)\n",
    "    \n",
    "    postfixes_a = [\"ها\", \"های\", \"هایی\", \"ی\", \"ان\",\n",
    "                   \"تر\", \"ترین\",\n",
    "                   \"گر\"]\n",
    "    postfixes_b = [\"\\u200bها\", \"\\u200cها\", \"\\u200bهای\", \"\\u200cهای\", \n",
    "                   \"\\u200bهایی\", \"\\u200cهایی\",\n",
    "                   \"\\u200bشده\", \"\\u200cشده\",\n",
    "                   \"\\u200bساز\", \"\\u200cساز\",\n",
    "                   \"\\u200bکنندگان\", \"\\u200cکنندگان\",\n",
    "                   \"\\u200bتر\", \"\\u200cتر\", \"\\u200bترین\", \"\\u200cترین\"]\n",
    "    \n",
    "    stemming_b = [\"می\\u200b\", \"می\\u200c\", \"نمی\\u200b\", \"نمی\\u200c\"]\n",
    "    stemming_e = [\"ات\", \"ام\", \"اش\", \n",
    "                  \"یم\", \"ید\", \"ند\", \n",
    "                  \"مان\", \"تان\", \"شان\"]\n",
    "    \n",
    "    prefixes = [\"با\", \"بی\", \"نا\"]\n",
    "    \n",
    "    # idea: avoid wrong cut with half space\n",
    "    token = re.sub(\"|\".join([f\"^{st}\" for st in stemming_b]), \" \", token)\n",
    "    token = re.sub(\"|\".join([f\"{pf}$\" for pf in postfixes_b]), \" \", token)\n",
    "    \n",
    "    if len(token) > 5: # idea: avoid wrong cut with char limit\n",
    "        token = re.sub(\"|\".join([f\"{pf}$\" for pf in postfixes_a]), \" \", token)\n",
    "        token = re.sub(\"|\".join([f\"{st}$\" for st in stemming_e]), \" \", token)\n",
    "        token = re.sub(\"|\".join([f\"^{pf}\" for pf in prefixes]), \" \", token)\n",
    "    \n",
    "    fa_digits = \"۱۲۳۴۵۶۷۸۹۰١٢٣٤٥٦٧٨٩٠\"\n",
    "    en_digits = \"12345678901234567890\"\n",
    "    token = token.translate(str.maketrans(fa_digits, en_digits))\n",
    "\n",
    "    token = re.sub(\"[ئي]\", \"ی\", token)\n",
    "    token = token.strip(\".ء \\u200b\\u200c\\u200d\")\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row):\n",
    "    tokens = np.array(row.content.split())\n",
    "    doc_id = np.full((len(tokens)), row.id)\n",
    "    return np.column_stack((tokens, doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.59 s, sys: 432 ms, total: 5.02 s\n",
      "Wall time: 5.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# saved\n",
    "tokenized_docs = []\n",
    "for tokens_doc in df.apply(tokenize, axis=1):\n",
    "    tokenized_docs.extend(tokens_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 10s, sys: 632 ms, total: 2min 10s\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# saved\n",
    "normalized_docs = []\n",
    "for [token, doc] in tokenized_docs:\n",
    "    token = normalize(normalize(token))\n",
    "    normalized_docs.append([token, doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.06 s, sys: 56 ms, total: 2.12 s\n",
      "Wall time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# saved\n",
    "tokens2docs = sorted(normalized_docs, key=lambda token_doc: token_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from 66985 items...\n",
      "CPU times: user 9.61 s, sys: 1.26 s, total: 10.9 s\n",
      "Wall time: 9.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# saved\n",
    "inverted_indexes = {}\n",
    "docs2tokens = {}\n",
    "print(f\"from {len(list(groupby(tokens2docs, key=lambda tokens2doc: tokens2doc[0])))} items...\")\n",
    "item = 0\n",
    "for [token, t_docs] in groupby(tokens2docs, key=lambda tokens2doc: tokens2doc[0]):\n",
    "    print(item, end=\"\\r\")\n",
    "    item += 1\n",
    "    docs = sorted(map(lambda item: int(item[1]), t_docs))\n",
    "    \n",
    "    tf = {}\n",
    "    for doc in docs:\n",
    "        if doc in tf:\n",
    "            tf[doc] += 1\n",
    "        else:\n",
    "            tf[doc] = 1\n",
    "    \n",
    "    if len(tf) > 1300:\n",
    "        continue\n",
    "    elif re.sub(\"\\s+\", \"\", token).isdigit() and len(re.sub(\"\\s+\", \"\", token)) < 4:\n",
    "        continue\n",
    "    elif re.match(\"^http|^https|^video\", token) and len(token) > 15:\n",
    "        continue\n",
    "    elif len(token) < 2:\n",
    "        continue\n",
    "    else:\n",
    "        for doc in tf.keys():\n",
    "            if doc in docs2tokens:\n",
    "                docs2tokens[doc] += [token]\n",
    "            else:\n",
    "                docs2tokens[doc] = [token]\n",
    "        inverted_indexes.update({ token: tf })\n",
    "\n",
    "inverted_indexes_file = open(\"inverted_indexes\", \"ab\")\n",
    "pickle.dump(inverted_indexes, inverted_indexes_file)\n",
    "inverted_indexes_file.close()\n",
    "\n",
    "docs2tokens_file = open(\"docs2tokens\", \"ab\")\n",
    "pickle.dump(docs2tokens, docs2tokens_file)\n",
    "docs2tokens_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_indexes_file = open(\"inverted_indexes\", \"rb\")\n",
    "inverted_indexes = pickle.load(inverted_indexes_file)\n",
    "\n",
    "docs2tokens_file = open(\"docs2tokens\", \"rb\")\n",
    "docs2tokens = pickle.load(docs2tokens_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    if row.id not in docs2tokens:\n",
    "        docs2tokens[row.id] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_dict = {token: math.log(len(df) / len(docs)) for token, docs in inverted_indexes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.4 ms, sys: 0 ns, total: 17.4 ms\n",
      "Wall time: 17.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "token2index = {}\n",
    "index2token = [0 for i in range(len(inverted_indexes))]\n",
    "for idx, token in enumerate(inverted_indexes.keys()):\n",
    "    token2index[token] = idx\n",
    "    index2token[idx] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 449 ms, sys: 24 ms, total: 473 ms\n",
      "Wall time: 472 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inverted_indexes_tf_idf = {}\n",
    "for [token, docs_dict] in inverted_indexes.items():\n",
    "    tf = {}\n",
    "    for [doc, freq] in docs_dict.items():\n",
    "        tf[doc] = (1 + math.log(freq)) * idf_dict[token]\n",
    "    inverted_indexes_tf_idf[token] = tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 549 ms, sys: 4.14 ms, total: 553 ms\n",
      "Wall time: 552 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def compute_length(doc_id):\n",
    "    length = 0\n",
    "    tokens = docs2tokens[doc_id]\n",
    "    for token in tokens:\n",
    "        length += inverted_indexes_tf_idf[token][doc_id]\n",
    "    return length\n",
    "df[\"length\"] = df[\"id\"].apply(compute_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved\n",
    "df_un_spvs[\"topic\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved\n",
    "df_un_spvs.to_csv(\"IR_Spring2021_ph12_7k.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved\n",
    "item = 0\n",
    "def knn(row):\n",
    "    global item\n",
    "    print(item, end=\"\\r\")\n",
    "    item += 1\n",
    "    \n",
    "    k = 5\n",
    "    tokens = []\n",
    "    for token in row.content.split():\n",
    "        tokens.append(normalize(normalize(token)))\n",
    "\n",
    "    q_tf = {}\n",
    "    for token in tokens:\n",
    "        if token in q_tf:\n",
    "            q_tf[token] += 1\n",
    "        else:\n",
    "            q_tf[token] = 1\n",
    "    \n",
    "    \n",
    "    result = {}\n",
    "    for token in tokens:\n",
    "        if token in inverted_indexes:\n",
    "            docs = inverted_indexes[token].keys()\n",
    "            doc_tf_idf = inverted_indexes_tf_idf[token]\n",
    "            q_tf_idf = (1 + math.log(q_tf[token])) * idf_dict[token]\n",
    "            \n",
    "            for doc in docs:              \n",
    "                if doc in result:\n",
    "                    result[doc] += doc_tf_idf[doc] * q_tf_idf\n",
    "                else:\n",
    "                    result.update({ doc: doc_tf_idf[doc] * q_tf_idf })\n",
    "              \n",
    "    for doc in result:\n",
    "        result[doc] /= df.loc[df[\"id\"] == doc, \"length\"].values[0]\n",
    "        \n",
    "    res_df = df.loc[df[\"id\"].isin(list(result.keys())), [\"id\", \"topic\"]].copy()\n",
    "    res_df[\"rank\"] = res_df[\"id\"].apply(lambda id: result[id])\n",
    "\n",
    "    final_df = res_df.loc[res_df[\"rank\"].isin(heapq.nlargest(k, list(res_df[\"rank\"])))].sort_values(by=[\"rank\"], ascending=False)    \n",
    "        \n",
    "    return max(set(list(final_df[\"topic\"])), key=list(final_df[\"topic\"]).count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6999\r"
     ]
    }
   ],
   "source": [
    "# saved\n",
    "df_un_spvs[\"topic\"] = df_un_spvs.apply(knn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved\n",
    "df_un_spvs.to_csv(\"IR_Spring2021_ph12_7k.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic      topic    \n",
       "culture    culture       326\n",
       "economy    economy      2125\n",
       "health     health       1609\n",
       "political  political    1182\n",
       "sport      sport        1758\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saved\n",
    "df_un_spvs.groupby(\"topic\")[\"topic\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_spvs_inverted_indexes_file = open(\"../Phase-2/inverted_indexes\", \"rb\")\n",
    "un_spvs_inverted_indexes = pickle.load(un_spvs_inverted_indexes_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_spvs_idf_dict = {token: math.log(len(df) / len(docs)) for token, docs in un_spvs_inverted_indexes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 295 ms, sys: 7.15 ms, total: 302 ms\n",
      "Wall time: 301 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "un_spvs_inverted_indexes_tf_idf = {}\n",
    "for [token, docs_dict] in un_spvs_inverted_indexes.items():\n",
    "    tf = {}\n",
    "    for [doc, freq] in docs_dict.items():\n",
    "        tf[doc] = (1 + math.log(freq)) * un_spvs_idf_dict[token]\n",
    "    un_spvs_inverted_indexes_tf_idf[token] = tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 14s, sys: 0 ns, total: 1min 14s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_doc_length(doc_id):\n",
    "    content = df_un_spvs.loc[df_un_spvs[\"id\"] == doc_id, \"content\"].values[0]\n",
    "    tokens = []\n",
    "    for token in content.split():\n",
    "        tokens.append(normalize(normalize(token)))\n",
    "    \n",
    "    length = 0\n",
    "    for token in tokens:\n",
    "        if token in un_spvs_inverted_indexes:\n",
    "            length += un_spvs_inverted_indexes[token][doc_id] ** 2\n",
    "    return math.sqrt(length)\n",
    "\n",
    "df_un_spvs[\"length\"] = df_un_spvs[\"id\"].apply(get_doc_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    k = 10\n",
    "    cat = None\n",
    "    \n",
    "    tokens = []\n",
    "    for token in query.split():\n",
    "        if \"cat:\" in token:\n",
    "            cat = token.split(\"cat:\")[1]\n",
    "        else:\n",
    "            tokens.append(normalize(normalize(token)))\n",
    "    \n",
    "    q_tf = {}\n",
    "    for token in tokens:\n",
    "        if token in q_tf:\n",
    "            q_tf[token] += 1\n",
    "        else:\n",
    "            q_tf[token] = 1\n",
    "            \n",
    "    result = {}\n",
    "    \n",
    "    found_token = []\n",
    "    for token in tokens:\n",
    "        if token in un_spvs_inverted_indexes:\n",
    "            found_token.append(token)\n",
    "            \n",
    "            docs = un_spvs_inverted_indexes[token].keys()\n",
    "            doc_tf_idf = un_spvs_inverted_indexes_tf_idf[token]\n",
    "            q_tf_idf = (1 + math.log(q_tf[token])) * un_spvs_idf_dict[token]\n",
    "            \n",
    "            for doc in docs:         \n",
    "                if doc in result:\n",
    "                    result[doc] += doc_tf_idf[doc] * q_tf_idf\n",
    "                else:\n",
    "                    result.update({ doc: doc_tf_idf[doc] * q_tf_idf })\n",
    "              \n",
    "    for doc in result:\n",
    "        result[doc] /= df_un_spvs.loc[df_un_spvs[\"id\"] == doc, \"length\"].values[0]\n",
    "    \n",
    "    res_df = df_un_spvs.loc[df_un_spvs[\"id\"].isin(list(result.keys())), [\"id\", \"url\", \"topic\"]].copy()\n",
    "    res_df[\"rank\"] = res_df[\"id\"].apply(lambda id: result[id])\n",
    "    \n",
    "    if cat:\n",
    "        res_df = res_df[res_df[\"topic\"] == cat]\n",
    "\n",
    "    final_df = res_df.loc[res_df[\"rank\"].isin(heapq.nlargest(k, list(res_df[\"rank\"])))].sort_values(by=[\"rank\"], ascending=False)\n",
    "    \n",
    "    print(found_token)\n",
    "    for index, row in final_df.iterrows():\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"#Doc {row.id}\")\n",
    "            print(row.url)\n",
    "            print(row[\"rank\"])\n",
    "            print(row.topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['انقلاب']\n",
      "================================================================================\n",
      "#Doc 2134\n",
      "https://www.isna.ir/news/99071007797/دستور-قضایی-بررسی-آتش-سوزی-بازارچه-ساحلی-بندر-دیلم-بازداشت-یک\n",
      "1.493588868639915\n",
      "economy\n",
      "================================================================================\n",
      "#Doc 4058\n",
      "https://www.isna.ir/news/99100100431/زمان-قطعی-برای-بررسی-بودجه-در-فراکسیون-انقلاب-اسلامی-تعیین-نشده\n",
      "1.1716925477371425\n",
      "economy\n",
      "================================================================================\n",
      "#Doc 4186\n",
      "https://www.isna.ir/news/99111511732/۲۶پروژه-با-۲۳۰۰میلیارد-تومان-در-کردستان-افتتاح-می-شود\n",
      "1.0551402829936243\n",
      "economy\n",
      "================================================================================\n",
      "#Doc 4209\n",
      "https://www.isna.ir/news/99112518369/اراضی-آموزش-و-پرورش-در-روستاها-و-شهرهای-کوچک-سند-دار-می-شود\n",
      "0.8271488361076161\n",
      "economy\n",
      "================================================================================\n",
      "#Doc 5121\n",
      "https://www.isna.ir/news/98090704741/هر-کدام-از-تحریم-های-اقتصادی-می-تواند-دولتی-را-سرنگون-کند\n",
      "0.8041730503900254\n",
      "economy\n",
      "================================================================================\n",
      "#Doc 4159\n",
      "https://www.isna.ir/news/99110503344/کدام-بخش-های-اقتصاد-پس-از-کرونا-اوج-می-گیرند\n",
      "0.7428574961865295\n",
      "economy\n",
      "================================================================================\n",
      "#Doc 4326\n",
      "https://www.isna.ir/news/98011907014/القای-جو-روانی-برای-افزایش-قیمت-ارز\n",
      "0.6906604581868157\n",
      "economy\n",
      "================================================================================\n",
      "#Doc 1983\n",
      "https://www.isna.ir/news/99052317223/پرونده-فرودگاه-قدیم-غار-نمکدان-و-پلاژ-بانوان-قشم-روی-میز-دادستان\n",
      "0.6013858335671262\n",
      "economy\n",
      "================================================================================\n",
      "#Doc 4411\n",
      "https://www.isna.ir/news/98021005490/خسارت-سیل-به-بیش-از-۱۹-هزار-واحد-مسکونی\n",
      "0.504292122040795\n",
      "economy\n",
      "================================================================================\n",
      "#Doc 4423\n",
      "https://www.isna.ir/news/98021306870/طول-خطوط-ریلی-ایران-امسال-چقدر-می-شود\n",
      "0.47514910492511603\n",
      "economy\n",
      "CPU times: user 114 ms, sys: 4.03 ms, total: 118 ms\n",
      "Wall time: 114 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# search(\"تمرینات تیم تکواندو\")\n",
    "# search(\"تیم ملی تکواندو\")\n",
    "# search(\"قهرمانی پرسپولیس cat:sport\")\n",
    "# search(\"کرونا cat:health\")\n",
    "# search(\"کرونا cat:economy\")\n",
    "# search(\"کرونا cat:political\")\n",
    "# search(\"انقلاب cat:culture\")\n",
    "search(\"انقلاب cat:economy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
