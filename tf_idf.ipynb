{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import groupby\n",
    "import math\n",
    "import heapq\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 214 ms, sys: 29.1 ms, total: 244 ms\n",
      "Wall time: 336 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# df = pd.read_excel(\"../IR_Spring2021_ph12_7k.xlsx\")\n",
    "df = pd.read_csv(\"IR_Spring2021_ph12_7k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(token):\n",
    "    token = token.strip(\".ء \\u200b\\u200c\\u200d\")\n",
    "    token = re.sub(\"[ـ،؛,–—!٪؟+:_٫/.]\", \" \", token)\n",
    "    token = re.sub(\"[*%&…#=\\\\\\•;!|\\-?]\", \" \", token)\n",
    "    token = re.sub(\"[﴾﴿«»()<>\\[\\]“”'\\\"]\", \" \", token)\n",
    "    token = re.sub(\"[\\u200f\\u200e\\ufeff\\u2067\\u202a\\u202b\\u202c\\u2069\\xad]\", \" \", token)\n",
    "    token = re.sub(\"\\s+\", \" \", token)\n",
    "    \n",
    "    plurals = {\n",
    "        \"آداب\": \"ادب\", \"اطراف\": \"طرف\", \"حقایق\": \"حقیقت\", \"امواج\": \"موج\",\n",
    "        \"مراکز\": \"مرکز\", \"اعماق\": \"عمق\", \"مواقع\": \"موقع\", \"اخبار\": \"خبر\",\n",
    "        \"علما\": \"عالم\", \"آثار\": \"اثر\", \"مصارف\": \"مصرف\", \"علوم\": \"علم\",\n",
    "        \"ادیان\": \"دین\", \"علائم\": \"علامت\", \"اسامی\": \"اسم\", \"مباحث\": \"مبحث\",\n",
    "        \"دفاتر\": \"دفتر\", \"علل\": \"علت\", \"مذاهب\": \"مذهب\", \"عناصر\": \"عنصر\",\n",
    "        \"مساجد\": \"مسجد\", \"روابط\": \"رابطه\", \"اعضا\": \"عضو\", \"عبارات\": \"عبارت\",\n",
    "        \"موارد\": \"مورد\", \"مفاهیم\": \"مفهوم\", \"اشعار\": \"اشعار\", \"منابع\": \"منبع\",\n",
    "        \"منبع\": \"قاعده\", \"فقها\": \"فقیه\", \"عجایب\": \"عجیب\", \"تصاویر\": \"تصویر\"\n",
    "    }\n",
    "    \n",
    "    for key, value in plurals.items():\n",
    "        if key in token:\n",
    "            token = token.replace(key, value)\n",
    "            \n",
    "    arabic_chars = ['\\u064B', '\\u064C', '\\u064D', '\\u064E', '\\u064F', '\\u0650', '\\u0651', '\\u0652']\n",
    "    token = re.sub(\"|\".join(arabic_chars), \"\", token)\n",
    "    \n",
    "    postfixes_a = [\"ها\", \"های\", \"هایی\", \"ی\", \"ان\",\n",
    "                   \"تر\", \"ترین\",\n",
    "                   \"گر\"]\n",
    "    postfixes_b = [\"\\u200bها\", \"\\u200cها\", \"\\u200bهای\", \"\\u200cهای\", \n",
    "                   \"\\u200bهایی\", \"\\u200cهایی\",\n",
    "                   \"\\u200bشده\", \"\\u200cشده\",\n",
    "                   \"\\u200bساز\", \"\\u200cساز\",\n",
    "                   \"\\u200bکنندگان\", \"\\u200cکنندگان\",\n",
    "                   \"\\u200bتر\", \"\\u200cتر\", \"\\u200bترین\", \"\\u200cترین\"]\n",
    "    \n",
    "    stemming_b = [\"می\\u200b\", \"می\\u200c\", \"نمی\\u200b\", \"نمی\\u200c\"]\n",
    "    stemming_e = [\"ات\", \"ام\", \"اش\", \n",
    "                  \"یم\", \"ید\", \"ند\", \n",
    "                  \"مان\", \"تان\", \"شان\"]\n",
    "    \n",
    "    prefixes = [\"با\", \"بی\", \"نا\"]\n",
    "    \n",
    "    # idea: avoid wrong cut with half space\n",
    "    token = re.sub(\"|\".join([f\"^{st}\" for st in stemming_b]), \" \", token)\n",
    "    token = re.sub(\"|\".join([f\"{pf}$\" for pf in postfixes_b]), \" \", token)\n",
    "    \n",
    "    if len(token) > 5: # idea: avoid wrong cut with char limit\n",
    "        token = re.sub(\"|\".join([f\"{pf}$\" for pf in postfixes_a]), \" \", token)\n",
    "        token = re.sub(\"|\".join([f\"{st}$\" for st in stemming_e]), \" \", token)\n",
    "        token = re.sub(\"|\".join([f\"^{pf}\" for pf in prefixes]), \" \", token)\n",
    "    \n",
    "    fa_digits = \"۱۲۳۴۵۶۷۸۹۰١٢٣٤٥٦٧٨٩٠\"\n",
    "    en_digits = \"12345678901234567890\"\n",
    "    token = token.translate(str.maketrans(fa_digits, en_digits))\n",
    "\n",
    "    token = re.sub(\"[ئي]\", \"ی\", token)\n",
    "    token = token.strip(\".ء \\u200b\\u200c\\u200d\")\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    tokens = np.array(doc.content.split())\n",
    "    doc_id = np.full((len(tokens)), doc.id)\n",
    "    return np.column_stack((tokens, doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.76 s, sys: 220 ms, total: 2.98 s\n",
      "Wall time: 2.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# saved\n",
    "tokenized_docs = []\n",
    "for tokens_doc in df.apply(tokenize, axis=1):\n",
    "    tokenized_docs.extend(tokens_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 19s, sys: 600 ms, total: 1min 19s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# saved\n",
    "normalized_docs = []\n",
    "for [token, doc] in tokenized_docs:\n",
    "    token = normalize(normalize(token))\n",
    "    normalized_docs.append([token, doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.22 s, sys: 16 ms, total: 1.23 s\n",
      "Wall time: 1.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# saved\n",
    "tokens_docs = sorted(normalized_docs, key=lambda token_doc: token_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.99 s, sys: 23.9 ms, total: 2.01 s\n",
      "Wall time: 2.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# saved\n",
    "inverted_indexes = {}\n",
    "for [token, t_docs] in groupby(tokens_docs, key=lambda tokens_doc: tokens_doc[0]):\n",
    "    docs = sorted(map(lambda item: int(item[1]), t_docs))\n",
    "    \n",
    "    tf = {}\n",
    "    for doc in docs:\n",
    "        if doc in tf:\n",
    "            tf[doc] += 1\n",
    "        else:\n",
    "            tf[doc] = 1\n",
    "    \n",
    "    if len(tf) > 1300:\n",
    "        continue\n",
    "    elif re.sub(\"\\s+\", \"\", token).isdigit() and len(re.sub(\"\\s+\", \"\", token)) < 4:\n",
    "        continue\n",
    "    elif re.match(\"^http|^https|^video\", token) and len(token) > 15:\n",
    "        continue\n",
    "    elif len(token) < 2:\n",
    "        continue\n",
    "    else:\n",
    "        inverted_indexes.update({ token: tf })\n",
    "        \n",
    "inverted_indexes_file = open(\"inverted_indexes\", \"ab\")\n",
    "pickle.dump(inverted_indexes, inverted_indexes_file)\n",
    "inverted_indexes_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_indexes_file = open(\"inverted_indexes\", \"rb\")\n",
    "inverted_indexes = pickle.load(inverted_indexes_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 16s, sys: 196 ms, total: 1min 17s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# saved\n",
    "def get_doc_length(doc_id):\n",
    "    content = df.loc[df[\"id\"] == doc_id, \"content\"].values[0]\n",
    "    tokens = []\n",
    "    for token in content.split():\n",
    "        tokens.append(normalize(normalize(token)))\n",
    "    \n",
    "    length = 0\n",
    "    for token in tokens:\n",
    "        if token in inverted_indexes:\n",
    "            length += inverted_indexes[token][doc_id] ** 2\n",
    "    return math.sqrt(length)\n",
    "\n",
    "df[\"length\"] = df[\"id\"].apply(get_doc_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved\n",
    "df.to_csv(\"IR_Spring2021_ph12_7k.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sort(input_list):\n",
    "    if len(input_list) > 1:\n",
    "        middle_index = len(input_list) // 2\n",
    "        \n",
    "        L = input_list[:middle_index]\n",
    "        R = input_list[middle_index:]\n",
    " \n",
    "        merge_sort(L)\n",
    "        merge_sort(R)\n",
    " \n",
    "        i = j = k = 0\n",
    " \n",
    "        while i < len(L) and j < len(R):\n",
    "            if L[i] < R[j]:\n",
    "                input_list[k] = L[i]\n",
    "                i += 1\n",
    "            else:\n",
    "                input_list[k] = R[j]\n",
    "                j += 1\n",
    "            k += 1\n",
    " \n",
    "        while i < len(L):\n",
    "            input_list[k] = L[i]\n",
    "            i += 1\n",
    "            k += 1\n",
    " \n",
    "        while j < len(R):\n",
    "            input_list[k] = R[j]\n",
    "            j += 1\n",
    "            k += 1\n",
    "    return input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    k = 7\n",
    "    with_heap = True\n",
    "    with_champion = True\n",
    "    \n",
    "    tokens = []\n",
    "    for token in query.split():\n",
    "        tokens.append(normalize(normalize(token)))\n",
    "    \n",
    "    q_tf = {}\n",
    "    for token in tokens:\n",
    "        if token in q_tf:\n",
    "            q_tf[token] += 1\n",
    "        else:\n",
    "            q_tf[token] = 1\n",
    "            \n",
    "    result = {}\n",
    "    \n",
    "    found_token = []\n",
    "    for token in tokens:\n",
    "        if token in inverted_indexes:\n",
    "            found_token.append(token)\n",
    "            doc_tf = inverted_indexes[token].items()\n",
    "            idf = math.log(len(df) / len(doc_tf)) \n",
    "            q_tf_idf = (1 + math.log(q_tf[token])) * idf\n",
    "            \n",
    "            if with_champion:\n",
    "                doc_tf = heapq.nlargest(k, doc_tf, key=lambda s: s[1])\n",
    "            \n",
    "            for doc, tf in doc_tf:              \n",
    "                tf_idf = (1 + math.log(tf)) * idf\n",
    "                if doc in result:\n",
    "                    result[doc] += tf_idf * q_tf_idf\n",
    "                else:\n",
    "                    result.update({ doc: tf_idf * q_tf_idf })\n",
    "              \n",
    "    for doc in result:\n",
    "        result[doc] /= df.loc[df[\"id\"] == doc, \"length\"].values[0]\n",
    "    \n",
    "    res_df = df.loc[df[\"id\"].isin(list(result.keys())), [\"id\", \"url\"]].copy()\n",
    "    res_df[\"rank\"] = res_df[\"id\"].apply(lambda id: result[id])\n",
    "\n",
    "    if with_heap:\n",
    "        final_df = res_df.loc[res_df[\"rank\"].isin(heapq.nlargest(k, list(res_df[\"rank\"])))].sort_values(by=[\"rank\"], ascending=False)\n",
    "    else:\n",
    "        final_df = res_df.loc[res_df[\"rank\"].isin(merge_sort(list(res_df[\"rank\"]))[-7:])].sort_values(by=[\"rank\"], ascending=False)\n",
    "    \n",
    "    print(found_token)\n",
    "    for index, row in final_df.iterrows():\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"#Doc {row.id}\")\n",
    "            print(row.url)\n",
    "            print(row[\"rank\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['تیم', 'تکواندو']\n",
      "================================================================================\n",
      "#Doc 62\n",
      "https://www.isna.ir/news/99012716113/برنامه-های-پیش-روی-تکواندو-در-گرو-جلسه-پولادگر-با-کادر-فنی-تیم\n",
      "1.6288028848677782\n",
      "================================================================================\n",
      "#Doc 818\n",
      "https://www.isna.ir/news/99101612523/علی-نژاد-خطاب-به-ملی-پوشان-تکواندو-در-المپیک-مدال-بگیرید-تا\n",
      "1.6103965304112329\n",
      "================================================================================\n",
      "#Doc 1291\n",
      "https://www.isna.ir/news/98052411781/رییس-فدراسیون-تکواندو-پرداختن-به-منافع-شخصی-همدلی-در-تکواندو\n",
      "1.4071763387606\n",
      "================================================================================\n",
      "#Doc 1130\n",
      "https://www.isna.ir/news/98030100267/یوسف-کرمی-پولادگر-هم-در-این-ناکامی-مقصر-است-چرا-از-قهرمانان\n",
      "1.1806699948918375\n",
      "================================================================================\n",
      "#Doc 650\n",
      "https://www.isna.ir/news/99081308613/یوسف-کرمی-سازمان-لیگ-تکواندو-حقی-برای-ورزشکار-قائل-نیست\n",
      "1.1188030834866889\n",
      "================================================================================\n",
      "#Doc 1288\n",
      "https://www.isna.ir/news/98052210720/سرمربی-تکواندوی-نونهالان-این-بچه-ها-مدال-آور-المپیک-می-شوند\n",
      "0.9754463750522372\n",
      "================================================================================\n",
      "#Doc 535\n",
      "https://www.isna.ir/news/99071511812/رییس-فدراسیون-جهانی-تکواندو-اولویت-ما-حفظ-سلامت-ورزشکاران-است\n",
      "0.913794347130121\n",
      "CPU times: user 28.1 ms, sys: 0 ns, total: 28.1 ms\n",
      "Wall time: 26 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# asasa = 'به گزارش ایسنا، فدراسیون بین المللی شنا قانونی را برای استفاده از فناوری ویدئو\\nدر زیر آب تصویب کرده است که در توکیو ۲۰۲۰ استفاده می شود. از این تجهیزات برای\\nتصمیمی که داور کنار استخر می گیرد استفاده می شود.\\n\\nبر این اساس در بازی های المپیک و مسابقات قهرمانی جهان از این تجهیزات استفاده\\nمی شود.\\n\\nاین فناوری تصمیم گیری جدیدی در مورد شناگران در مسابقات ایجاد نمی کند فقط\\nتصمیمات داور کنار استخر را تائید یا لغو می کند.\\n\\nاین فناوری برای تایید یا لغو تصمیمات داور کنار استخر استفاده خواهد شد. شناگران\\nموظفند از این قوانین پیروی کنند.  \\nاین فناوری بعد از تصویب در المپیک توکیو ۲۰۲۰ استفاده می شود.\\n\\nانتهای پیام\\n\\n'\n",
    "# search(asasa)\n",
    "\n",
    "search(\"تیم ملی تکواندو\")\n",
    "# search(\"قهرمانی پرسپولیس\")\n",
    "# search(\"تمرینات تیم تکواندو\")\n",
    "\n",
    "# search(\"قیمت‌گذاری کالاهای صادراتی\")\n",
    "# search(\"وام‌های اشتغال‌زایی عشایری و روستایی\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
